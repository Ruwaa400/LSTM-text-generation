{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Arabic - LSTM textGen with GPU.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdxnHcoujl7o"
      },
      "source": [
        "# Larger LSTM Network to Generate Text for \"Men in The Sun\"\n",
        "import numpy\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLOVgfcYV9yC"
      },
      "source": [
        "# load ascii text and covert to lowercase\n",
        "filename = \"men_in_the_sun_edited.txt\"\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "raw_text = raw_text.lower()\n",
        "\n",
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WZOdMxOV-Da",
        "outputId": "ff37718e-3ccb-494b-b412-30b2b4588744",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# summarize the loaded data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print (\"Total Characters: \", n_chars)\n",
        "print (\"Total Vocab: \", n_vocab)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  71818\n",
            "Total Vocab:  61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-BziWetV-Kg",
        "outputId": "b5785569-f1b2-4d7b-dd4d-018ce03197d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "    seq_in = raw_text[i:i + seq_length]\n",
        "    seq_out = raw_text[i + seq_length]\n",
        "    dataX.append([char_to_int[char] for char in seq_in])\n",
        "    dataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print (\"Total Patterns: \", n_patterns)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  71718\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wr5O2CzV-Rs"
      },
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = to_categorical(dataY)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNCQmSqqV-Vr",
        "outputId": "f4efc8c3-cd1a-4d96-da9a-a03eff62fcce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(LSTM(512, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(512, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(512, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(512))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "print(model.summary())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_15 (LSTM)               (None, 100, 256)          264192    \n",
            "_________________________________________________________________\n",
            "lstm_16 (LSTM)               (None, 100, 512)          1574912   \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 100, 512)          0         \n",
            "_________________________________________________________________\n",
            "lstm_17 (LSTM)               (None, 100, 512)          2099200   \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 100, 512)          0         \n",
            "_________________________________________________________________\n",
            "lstm_18 (LSTM)               (None, 100, 512)          2099200   \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 100, 512)          0         \n",
            "_________________________________________________________________\n",
            "lstm_19 (LSTM)               (None, 512)               2099200   \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 61)                31293     \n",
            "=================================================================\n",
            "Total params: 8,167,997\n",
            "Trainable params: 8,167,997\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJ6GjQQaSa8T"
      },
      "source": [
        "# compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZB3kzqIV-hp"
      },
      "source": [
        "# define the checkpoint\n",
        "filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xd0aMqALV-fd",
        "outputId": "08fab7d2-f20d-49a3-ee35-84d488dbc924",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# fit the model\n",
        "model.fit(X, y, epochs=60, batch_size=512, callbacks=callbacks_list)\n",
        "# model.fit(X, y, batch_size=512, epochs=50)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "  2/141 [..............................] - ETA: 1:16 - loss: 4.0468WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2801s vs `on_train_batch_end` time: 0.4749s). Check your callbacks.\n",
            "141/141 [==============================] - ETA: 0s - loss: 3.2024\n",
            "Epoch 00001: loss improved from inf to 3.20243, saving model to weights-improvement-01-3.2024-bigger.hdf5\n",
            "141/141 [==============================] - 107s 762ms/step - loss: 3.2024\n",
            "Epoch 2/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 3.1582\n",
            "Epoch 00002: loss improved from 3.20243 to 3.15818, saving model to weights-improvement-02-3.1582-bigger.hdf5\n",
            "141/141 [==============================] - 109s 773ms/step - loss: 3.1582\n",
            "Epoch 3/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 3.1545\n",
            "Epoch 00003: loss improved from 3.15818 to 3.15446, saving model to weights-improvement-03-3.1545-bigger.hdf5\n",
            "141/141 [==============================] - 110s 779ms/step - loss: 3.1545\n",
            "Epoch 4/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 3.1509\n",
            "Epoch 00004: loss improved from 3.15446 to 3.15090, saving model to weights-improvement-04-3.1509-bigger.hdf5\n",
            "141/141 [==============================] - 110s 782ms/step - loss: 3.1509\n",
            "Epoch 5/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 3.1497\n",
            "Epoch 00005: loss improved from 3.15090 to 3.14968, saving model to weights-improvement-05-3.1497-bigger.hdf5\n",
            "141/141 [==============================] - 110s 783ms/step - loss: 3.1497\n",
            "Epoch 6/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 3.1438\n",
            "Epoch 00006: loss improved from 3.14968 to 3.14380, saving model to weights-improvement-06-3.1438-bigger.hdf5\n",
            "141/141 [==============================] - 111s 784ms/step - loss: 3.1438\n",
            "Epoch 7/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 2.9548\n",
            "Epoch 00007: loss improved from 3.14380 to 2.95477, saving model to weights-improvement-07-2.9548-bigger.hdf5\n",
            "141/141 [==============================] - 111s 786ms/step - loss: 2.9548\n",
            "Epoch 8/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 2.7855\n",
            "Epoch 00008: loss improved from 2.95477 to 2.78550, saving model to weights-improvement-08-2.7855-bigger.hdf5\n",
            "141/141 [==============================] - 111s 789ms/step - loss: 2.7855\n",
            "Epoch 9/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 2.6733\n",
            "Epoch 00009: loss improved from 2.78550 to 2.67329, saving model to weights-improvement-09-2.6733-bigger.hdf5\n",
            "141/141 [==============================] - 111s 789ms/step - loss: 2.6733\n",
            "Epoch 10/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 2.5684\n",
            "Epoch 00010: loss improved from 2.67329 to 2.56842, saving model to weights-improvement-10-2.5684-bigger.hdf5\n",
            "141/141 [==============================] - 111s 788ms/step - loss: 2.5684\n",
            "Epoch 11/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 2.4910\n",
            "Epoch 00011: loss improved from 2.56842 to 2.49099, saving model to weights-improvement-11-2.4910-bigger.hdf5\n",
            "141/141 [==============================] - 111s 789ms/step - loss: 2.4910\n",
            "Epoch 12/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 2.4198\n",
            "Epoch 00012: loss improved from 2.49099 to 2.41982, saving model to weights-improvement-12-2.4198-bigger.hdf5\n",
            "141/141 [==============================] - 111s 788ms/step - loss: 2.4198\n",
            "Epoch 13/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 2.3582\n",
            "Epoch 00013: loss improved from 2.41982 to 2.35820, saving model to weights-improvement-13-2.3582-bigger.hdf5\n",
            "141/141 [==============================] - 111s 791ms/step - loss: 2.3582\n",
            "Epoch 14/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 2.2894\n",
            "Epoch 00014: loss improved from 2.35820 to 2.28941, saving model to weights-improvement-14-2.2894-bigger.hdf5\n",
            "141/141 [==============================] - 112s 792ms/step - loss: 2.2894\n",
            "Epoch 15/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 2.2221\n",
            "Epoch 00015: loss improved from 2.28941 to 2.22213, saving model to weights-improvement-15-2.2221-bigger.hdf5\n",
            "141/141 [==============================] - 111s 791ms/step - loss: 2.2221\n",
            "Epoch 16/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 2.1495\n",
            "Epoch 00016: loss improved from 2.22213 to 2.14949, saving model to weights-improvement-16-2.1495-bigger.hdf5\n",
            "141/141 [==============================] - 111s 790ms/step - loss: 2.1495\n",
            "Epoch 17/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 2.0720\n",
            "Epoch 00017: loss improved from 2.14949 to 2.07201, saving model to weights-improvement-17-2.0720-bigger.hdf5\n",
            "141/141 [==============================] - 111s 791ms/step - loss: 2.0720\n",
            "Epoch 18/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 1.9940\n",
            "Epoch 00018: loss improved from 2.07201 to 1.99397, saving model to weights-improvement-18-1.9940-bigger.hdf5\n",
            "141/141 [==============================] - 112s 791ms/step - loss: 1.9940\n",
            "Epoch 19/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 1.9061\n",
            "Epoch 00019: loss improved from 1.99397 to 1.90611, saving model to weights-improvement-19-1.9061-bigger.hdf5\n",
            "141/141 [==============================] - 112s 798ms/step - loss: 1.9061\n",
            "Epoch 20/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 1.8347\n",
            "Epoch 00020: loss improved from 1.90611 to 1.83469, saving model to weights-improvement-20-1.8347-bigger.hdf5\n",
            "141/141 [==============================] - 112s 794ms/step - loss: 1.8347\n",
            "Epoch 21/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 1.7418\n",
            "Epoch 00021: loss improved from 1.83469 to 1.74183, saving model to weights-improvement-21-1.7418-bigger.hdf5\n",
            "141/141 [==============================] - 111s 790ms/step - loss: 1.7418\n",
            "Epoch 22/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 1.6372\n",
            "Epoch 00022: loss improved from 1.74183 to 1.63718, saving model to weights-improvement-22-1.6372-bigger.hdf5\n",
            "141/141 [==============================] - 111s 790ms/step - loss: 1.6372\n",
            "Epoch 23/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 1.5334\n",
            "Epoch 00023: loss improved from 1.63718 to 1.53343, saving model to weights-improvement-23-1.5334-bigger.hdf5\n",
            "141/141 [==============================] - 111s 790ms/step - loss: 1.5334\n",
            "Epoch 24/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 1.4321\n",
            "Epoch 00024: loss improved from 1.53343 to 1.43206, saving model to weights-improvement-24-1.4321-bigger.hdf5\n",
            "141/141 [==============================] - 112s 791ms/step - loss: 1.4321\n",
            "Epoch 25/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 1.3334\n",
            "Epoch 00025: loss improved from 1.43206 to 1.33336, saving model to weights-improvement-25-1.3334-bigger.hdf5\n",
            "141/141 [==============================] - 111s 791ms/step - loss: 1.3334\n",
            "Epoch 26/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 1.2471\n",
            "Epoch 00026: loss improved from 1.33336 to 1.24708, saving model to weights-improvement-26-1.2471-bigger.hdf5\n",
            "141/141 [==============================] - 111s 791ms/step - loss: 1.2471\n",
            "Epoch 27/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 1.1366\n",
            "Epoch 00027: loss improved from 1.24708 to 1.13660, saving model to weights-improvement-27-1.1366-bigger.hdf5\n",
            "141/141 [==============================] - 112s 792ms/step - loss: 1.1366\n",
            "Epoch 28/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 1.0465\n",
            "Epoch 00028: loss improved from 1.13660 to 1.04652, saving model to weights-improvement-28-1.0465-bigger.hdf5\n",
            "141/141 [==============================] - 111s 790ms/step - loss: 1.0465\n",
            "Epoch 29/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 0.9688\n",
            "Epoch 00029: loss improved from 1.04652 to 0.96881, saving model to weights-improvement-29-0.9688-bigger.hdf5\n",
            "141/141 [==============================] - 111s 790ms/step - loss: 0.9688\n",
            "Epoch 30/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 0.8935\n",
            "Epoch 00030: loss improved from 0.96881 to 0.89353, saving model to weights-improvement-30-0.8935-bigger.hdf5\n",
            "141/141 [==============================] - 111s 790ms/step - loss: 0.8935\n",
            "Epoch 31/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 0.8103\n",
            "Epoch 00031: loss improved from 0.89353 to 0.81032, saving model to weights-improvement-31-0.8103-bigger.hdf5\n",
            "141/141 [==============================] - 111s 789ms/step - loss: 0.8103\n",
            "Epoch 32/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 0.7391\n",
            "Epoch 00032: loss improved from 0.81032 to 0.73909, saving model to weights-improvement-32-0.7391-bigger.hdf5\n",
            "141/141 [==============================] - 111s 790ms/step - loss: 0.7391\n",
            "Epoch 33/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 0.6773\n",
            "Epoch 00033: loss improved from 0.73909 to 0.67726, saving model to weights-improvement-33-0.6773-bigger.hdf5\n",
            "141/141 [==============================] - 111s 790ms/step - loss: 0.6773\n",
            "Epoch 34/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 0.6217\n",
            "Epoch 00034: loss improved from 0.67726 to 0.62170, saving model to weights-improvement-34-0.6217-bigger.hdf5\n",
            "141/141 [==============================] - 111s 790ms/step - loss: 0.6217\n",
            "Epoch 35/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 0.5634\n",
            "Epoch 00035: loss improved from 0.62170 to 0.56337, saving model to weights-improvement-35-0.5634-bigger.hdf5\n",
            "141/141 [==============================] - 111s 790ms/step - loss: 0.5634\n",
            "Epoch 36/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 0.5387\n",
            "Epoch 00036: loss improved from 0.56337 to 0.53866, saving model to weights-improvement-36-0.5387-bigger.hdf5\n",
            "141/141 [==============================] - 111s 790ms/step - loss: 0.5387\n",
            "Epoch 37/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 0.4772\n",
            "Epoch 00037: loss improved from 0.53866 to 0.47722, saving model to weights-improvement-37-0.4772-bigger.hdf5\n",
            "141/141 [==============================] - 111s 790ms/step - loss: 0.4772\n",
            "Epoch 38/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 0.4393\n",
            "Epoch 00038: loss improved from 0.47722 to 0.43934, saving model to weights-improvement-38-0.4393-bigger.hdf5\n",
            "141/141 [==============================] - 111s 790ms/step - loss: 0.4393\n",
            "Epoch 39/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 0.4112\n",
            "Epoch 00039: loss improved from 0.43934 to 0.41119, saving model to weights-improvement-39-0.4112-bigger.hdf5\n",
            "141/141 [==============================] - 112s 796ms/step - loss: 0.4112\n",
            "Epoch 40/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 0.4842\n",
            "Epoch 00040: loss did not improve from 0.41119\n",
            "141/141 [==============================] - 112s 791ms/step - loss: 0.4842\n",
            "Epoch 41/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 0.3743\n",
            "Epoch 00041: loss improved from 0.41119 to 0.37434, saving model to weights-improvement-41-0.3743-bigger.hdf5\n",
            "141/141 [==============================] - 112s 792ms/step - loss: 0.3743\n",
            "Epoch 42/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 0.3228\n",
            "Epoch 00042: loss improved from 0.37434 to 0.32283, saving model to weights-improvement-42-0.3228-bigger.hdf5\n",
            "141/141 [==============================] - 112s 791ms/step - loss: 0.3228\n",
            "Epoch 43/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 0.3090\n",
            "Epoch 00043: loss improved from 0.32283 to 0.30897, saving model to weights-improvement-43-0.3090-bigger.hdf5\n",
            "141/141 [==============================] - 111s 790ms/step - loss: 0.3090\n",
            "Epoch 44/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 0.2926\n",
            "Epoch 00044: loss improved from 0.30897 to 0.29259, saving model to weights-improvement-44-0.2926-bigger.hdf5\n",
            "141/141 [==============================] - 111s 790ms/step - loss: 0.2926\n",
            "Epoch 45/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 0.2767\n",
            "Epoch 00045: loss improved from 0.29259 to 0.27672, saving model to weights-improvement-45-0.2767-bigger.hdf5\n",
            "141/141 [==============================] - 112s 791ms/step - loss: 0.2767\n",
            "Epoch 46/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 0.2562\n",
            "Epoch 00046: loss improved from 0.27672 to 0.25616, saving model to weights-improvement-46-0.2562-bigger.hdf5\n",
            "141/141 [==============================] - 111s 791ms/step - loss: 0.2562\n",
            "Epoch 47/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 0.2428\n",
            "Epoch 00047: loss improved from 0.25616 to 0.24277, saving model to weights-improvement-47-0.2428-bigger.hdf5\n",
            "141/141 [==============================] - 111s 791ms/step - loss: 0.2428\n",
            "Epoch 48/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 0.2373\n",
            "Epoch 00048: loss improved from 0.24277 to 0.23731, saving model to weights-improvement-48-0.2373-bigger.hdf5\n",
            "141/141 [==============================] - 112s 791ms/step - loss: 0.2373\n",
            "Epoch 49/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 0.2277\n",
            "Epoch 00049: loss improved from 0.23731 to 0.22775, saving model to weights-improvement-49-0.2277-bigger.hdf5\n",
            "141/141 [==============================] - 112s 791ms/step - loss: 0.2277\n",
            "Epoch 50/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 0.2208\n",
            "Epoch 00050: loss improved from 0.22775 to 0.22084, saving model to weights-improvement-50-0.2208-bigger.hdf5\n",
            "141/141 [==============================] - 112s 792ms/step - loss: 0.2208\n",
            "Epoch 51/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 0.2028\n",
            "Epoch 00051: loss improved from 0.22084 to 0.20281, saving model to weights-improvement-51-0.2028-bigger.hdf5\n",
            "141/141 [==============================] - 112s 793ms/step - loss: 0.2028\n",
            "Epoch 52/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 0.2142\n",
            "Epoch 00052: loss did not improve from 0.20281\n",
            "141/141 [==============================] - 111s 788ms/step - loss: 0.2142\n",
            "Epoch 53/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 0.1946\n",
            "Epoch 00053: loss improved from 0.20281 to 0.19457, saving model to weights-improvement-53-0.1946-bigger.hdf5\n",
            "141/141 [==============================] - 111s 791ms/step - loss: 0.1946\n",
            "Epoch 54/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 0.1982\n",
            "Epoch 00054: loss did not improve from 0.19457\n",
            "141/141 [==============================] - 111s 788ms/step - loss: 0.1982\n",
            "Epoch 55/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 0.1950\n",
            "Epoch 00055: loss did not improve from 0.19457\n",
            "141/141 [==============================] - 112s 791ms/step - loss: 0.1950\n",
            "Epoch 56/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 0.1727\n",
            "Epoch 00056: loss improved from 0.19457 to 0.17268, saving model to weights-improvement-56-0.1727-bigger.hdf5\n",
            "141/141 [==============================] - 112s 792ms/step - loss: 0.1727\n",
            "Epoch 57/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 0.1716\n",
            "Epoch 00057: loss improved from 0.17268 to 0.17159, saving model to weights-improvement-57-0.1716-bigger.hdf5\n",
            "141/141 [==============================] - 112s 792ms/step - loss: 0.1716\n",
            "Epoch 58/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 0.1775\n",
            "Epoch 00058: loss did not improve from 0.17159\n",
            "141/141 [==============================] - 111s 790ms/step - loss: 0.1775\n",
            "Epoch 59/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 0.1653\n",
            "Epoch 00059: loss improved from 0.17159 to 0.16527, saving model to weights-improvement-59-0.1653-bigger.hdf5\n",
            "141/141 [==============================] - 112s 796ms/step - loss: 0.1653\n",
            "Epoch 60/60\n",
            "141/141 [==============================] - ETA: 0s - loss: 0.1701\n",
            "Epoch 00060: loss did not improve from 0.16527\n",
            "141/141 [==============================] - 112s 795ms/step - loss: 0.1701\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7bfe392710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    }
  ]
}